{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1504f561",
   "metadata": {},
   "source": [
    "# Milestone-2: AirAware ML Training Pipeline\n",
    "\n",
    "This notebook implements the machine learning pipeline for predicting PM2.5 values based on air quality features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5658ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import math\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cbbc6e",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe66006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned dataset\n",
    "df = pd.read_csv('../data/cleaned/delhi_air_quality_cleaned.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b7165a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features as per requirements\n",
    "# Note: We need to map the available columns to the required features\n",
    "# Available columns: Date,Month,Year,Holidays_Count,Days,PM2.5,PM10,NO2,SO2,CO,Ozone,AQI\n",
    "# Required features: pm25, pm10, co2, no2, so2, temperature, humidity\n",
    "\n",
    "# Since we don't have temperature and humidity in the dataset, \n",
    "# we'll use PM2.5, PM10, NO2, SO2, CO for modeling\n",
    "# For CO2, we'll use CO values (as a proxy)\n",
    "\n",
    "# Rename columns to match required feature names\n",
    "df_features = df[['PM2.5', 'PM10', 'CO', 'NO2', 'SO2']].copy()\n",
    "df_features.columns = ['pm25', 'pm10', 'co2', 'no2', 'so2']\n",
    "\n",
    "# Add dummy temperature and humidity values (since they're not in the dataset)\n",
    "# In a real scenario, these would come from weather data\n",
    "np.random.seed(42)  # For reproducibility\n",
    "df_features['temperature'] = np.random.normal(25, 5, len(df_features))  # Mean 25°C, std 5°C\n",
    "df_features['humidity'] = np.random.normal(60, 15, len(df_features))    # Mean 60%, std 15%\n",
    "\n",
    "print(\"Feature dataset shape:\", df_features.shape)\n",
    "df_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da888812",
   "metadata": {},
   "source": [
    "## 2. Create Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d044116b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target variable: Predict next PM2.5 value\n",
    "df_features[\"target_pm25\"] = df_features[\"pm25\"].shift(-1)\n",
    "\n",
    "# Drop rows with NaN values\n",
    "df_features.dropna(inplace=True)\n",
    "\n",
    "print(f\"Dataset shape after creating target: {df_features.shape}\")\n",
    "df_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6728d06d",
   "metadata": {},
   "source": [
    "## 3. Train/Test Split (80/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbf7205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "feature_columns = ['pm25', 'pm10', 'co2', 'no2', 'so2', 'temperature', 'humidity']\n",
    "X = df_features[feature_columns]\n",
    "y = df_features[\"target_pm25\"]\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape}\")\n",
    "print(f\"Testing set size: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d266c42b",
   "metadata": {},
   "source": [
    "## 4. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbfb61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'Support Vector Machine': SVR(kernel='rbf')\n",
    "}\n",
    "\n",
    "# Train models and store predictions\n",
    "trained_models = {}\n",
    "predictions = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    trained_models[name] = model\n",
    "    predictions[name] = model.predict(X_test)\n",
    "    print(f\"{name} training completed.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf325f4",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d3ede0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate models\n",
    "evaluation_results = {}\n",
    "\n",
    "for name, y_pred in predictions.items():\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = math.sqrt(mse)\n",
    "    \n",
    "    evaluation_results[name] = {\n",
    "        'R2_Score': r2,\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse\n",
    "    }\n",
    "    \n",
    "    print(f\"{name} Results:\")\n",
    "    print(f\"  R² Score: {r2:.4f}\")\n",
    "    print(f\"  MSE: {mse:.4f}\")\n",
    "    print(f\"  RMSE: {rmse:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92936ef2",
   "metadata": {},
   "source": [
    "## 6. Identify Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d942349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best model based on R² score\n",
    "best_model_name = max(evaluation_results, key=lambda x: evaluation_results[x]['R2_Score'])\n",
    "best_model = trained_models[best_model_name]\n",
    "\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"R² Score: {evaluation_results[best_model_name]['R2_Score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0dbf81",
   "metadata": {},
   "source": [
    "## 7. Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e13ae0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory if it doesn't exist\n",
    "os.makedirs('../backend/models', exist_ok=True)\n",
    "\n",
    "# Save the best model\n",
    "model_path = '../backend/models/best_pm25_model.pkl'\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "\n",
    "print(f\"Best model ({best_model_name}) saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b1fd43",
   "metadata": {},
   "source": [
    "## 8. Generate Actual vs Predicted Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6936c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions from the best model\n",
    "best_predictions = predictions[best_model_name]\n",
    "\n",
    "# Store actual vs predicted values\n",
    "actual_vs_predicted = {\n",
    "    'actual': y_test.values.tolist(),\n",
    "    'predicted': best_predictions.tolist()\n",
    "}\n",
    "\n",
    "print(f\"Number of actual values: {len(actual_vs_predicted['actual'])}\")\n",
    "print(f\"Number of predicted values: {len(actual_vs_predicted['predicted'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb38d5a",
   "metadata": {},
   "source": [
    "## 9. Calculate Accuracy Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9988d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy metrics for the best model\n",
    "accuracy_values = evaluation_results[best_model_name]\n",
    "print(\"Accuracy values for the best model:\")\n",
    "for metric, value in accuracy_values.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2855813c",
   "metadata": {},
   "source": [
    "## 10. Calculate Error-Percentage Matrix for Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccf9ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate error percentages\n",
    "actual_vals = np.array(actual_vs_predicted['actual'])\n",
    "predicted_vals = np.array(actual_vs_predicted['predicted'])\n",
    "\n",
    "# Calculate percentage error (avoiding division by zero)\n",
    "percentage_errors = np.abs((actual_vals - predicted_vals) / np.where(actual_vals != 0, actual_vals, 1)) * 100\n",
    "\n",
    "# Create error matrix for heatmap visualization\n",
    "error_matrix = {\n",
    "    'actual': actual_vals.tolist(),\n",
    "    'predicted': predicted_vals.tolist(),\n",
    "    'percentage_error': percentage_errors.tolist()\n",
    "}\n",
    "\n",
    "print(f\"Calculated {len(percentage_errors)} error percentages\")\n",
    "print(f\"Mean percentage error: {np.mean(percentage_errors):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1183a4",
   "metadata": {},
   "source": [
    "## 11. Export Comparison Dataset as JSON for Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549e6910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataset\n",
    "comparison_data = []\n",
    "for i in range(len(actual_vals)):\n",
    "    comparison_data.append({\n",
    "        'index': i,\n",
    "        'actual': float(actual_vals[i]),\n",
    "        'predicted': float(predicted_vals[i]),\n",
    "        'percentage_error': float(percentage_errors[i])\n",
    "    })\n",
    "\n",
    "# Save as JSON\n",
    "comparison_json_path = '../data/cleaned/comparison_data.json'\n",
    "with open(comparison_json_path, 'w') as f:\n",
    "    json.dump(comparison_data, f, indent=2)\n",
    "\n",
    "print(f\"Comparison dataset exported to {comparison_json_path}\")\n",
    "print(f\"Total records exported: {len(comparison_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2d729c",
   "metadata": {},
   "source": [
    "## 12. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842ede42",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== MILESTONE-2 SUMMARY ===\")\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"R² Score: {evaluation_results[best_model_name]['R2_Score']:.4f}\")\n",
    "print(f\"MSE: {evaluation_results[best_model_name]['MSE']:.4f}\")\n",
    "print(f\"RMSE: {evaluation_results[best_model_name]['RMSE']:.4f}\")\n",
    "print(f\"Model saved to: ../backend/models/best_pm25_model.pkl\")\n",
    "print(f\"Comparison data exported to: ../data/cleaned/comparison_data.json\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
